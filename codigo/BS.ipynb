{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bb9242",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Distintos tipos de bandwith para ir probando\n",
    "dict_bandwith = {\n",
    "    'normal_mean': ((4/(3*len(raw_data)))**(1/5)) * np.std(raw_data),\n",
    "    'normal_mean_iqr': ((4/(3*len(raw_data)))**(1/5)) * (np.percentile(raw_data, 75) - np.percentile(raw_data, 25)),\n",
    "    \"scott\": 1.06 * np.std(raw_data) * (len(raw_data) ** (-1 / 5))\n",
    "}\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad53bff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo internet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.stats import norm, lognorm, gamma, poisson, chisquare, kstest, pearsonr\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "rand_seed = 100\n",
    "\n",
    "def make_data_normal(data_count=100):\n",
    "    np.random.seed(rand_seed)\n",
    "    x = np.random.normal(0, 1, data_count)\n",
    "    dist = lambda z: stats.norm(0, 1).pdf(z)\n",
    "    return x, dist\n",
    "\n",
    "def make_data_binormal(data_count=100):\n",
    "    alpha = 0.3\n",
    "    np.random.seed(rand_seed)\n",
    "    x = np.concatenate([\n",
    "        np.random.normal(-1, 2, int(data_count * alpha)),\n",
    "        np.random.normal(5, 1, int(data_count * (1 - alpha)))\n",
    "    ])\n",
    "    dist = lambda z: alpha * stats.norm(-1, 2).pdf(z) + (1 - alpha) * stats.norm(5, 1).pdf(z)\n",
    "    return x, dist\n",
    "\n",
    "def make_data_exp(data_count=100):\n",
    "    alpha = 0.3\n",
    "    np.random.seed(rand_seed)\n",
    "    x = np.concatenate([\n",
    "        np.random.exponential(1, int(data_count * alpha)),\n",
    "        np.random.exponential(1, int(data_count * (1 - alpha))) + 1\n",
    "    ])\n",
    "    dist = lambda z: alpha * stats.expon(0).pdf(z) + (1 - alpha) * stats.expon(1).pdf(z)\n",
    "    return x, dist\n",
    "\n",
    "def make_data_uniform(data_count=100):\n",
    "    alpha = 0.3\n",
    "    np.random.seed(rand_seed)\n",
    "    x = np.concatenate([\n",
    "        np.random.uniform(-1, 1, int(data_count * alpha)),\n",
    "        np.random.uniform(0, 1, int(data_count * (1 - alpha)))\n",
    "    ])\n",
    "    dist = lambda z: alpha * stats.uniform(-1, 1).pdf(z) + (1 - alpha) * stats.uniform(0, 1).pdf(z)\n",
    "    return x, dist\n",
    "\n",
    "x_norm, dist_norm = make_data_normal()\n",
    "x_binorm, dist_binorm = make_data_binormal()\n",
    "x_exp, dist_exp = make_data_exp()\n",
    "x_uni, dist_uni = make_data_uniform()\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 3))\n",
    "names = ['Normal', 'Bi-normal', 'Exponential', 'Bi-Uniform']\n",
    "for i, d in enumerate([dist_norm, dist_binorm, dist_exp, dist_uni]):\n",
    "    x = np.linspace(-8, 8, 100)\n",
    "    ax[i].fill(x, d(x), color='C0', alpha=0.5)\n",
    "    ax[i].set_ylim(0, 1)\n",
    "    ax[i].set_xlim(-8, 8)\n",
    "    ax[i].set_xlabel('x')\n",
    "    ax[i].set_ylabel('p(x)')\n",
    "    ax[i].set_title(names[i])\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "def kernel(k: str):\n",
    "    \"\"\"Kernel Functions.\n",
    "    Ref: https://en.wikipedia.org/wiki/Kernel_(statistics)\n",
    "\n",
    "    Args:\n",
    "        k (str): Kernel name. Can be one of ['gaussian', 'epanechnikov', 'cosine', 'linear'.]\n",
    "    \"\"\"\n",
    "    \n",
    "    if k not in ['gaussian', 'epanechnikov', 'cosine', 'linear']:\n",
    "        raise ValueError('Unknown kernel.')\n",
    "\n",
    "    def bounded(f):\n",
    "        def _f(x):\n",
    "            return f(x) if np.abs(x) <= 1 else 0\n",
    "        return _f\n",
    "\n",
    "    if k == 'gaussian':\n",
    "        return lambda u: 1 / np.sqrt(2 * np.pi) * np.exp(-1 / 2 * u * u)\n",
    "    elif k == 'epanechnikov':\n",
    "        return bounded(lambda u: (3 / 4 * (1 - u * u)))\n",
    "    elif k =='cosine':\n",
    "        return bounded(lambda u: np.pi / 4 * np.cos(np.pi / 2 * u))\n",
    "    elif k == 'linear':\n",
    "        return bounded(lambda u: 1 - np.abs(u))\n",
    "    \n",
    "def bw_scott(data: np.ndarray):\n",
    "    std_dev = np.std(data, axis=0, ddof=1)\n",
    "    n = len(data)\n",
    "    return 3.49 * std_dev * n ** (-0.333)\n",
    "\n",
    "def bw_silverman(data: np.ndarray):\n",
    "    def _select_sigma(x):\n",
    "        normalizer = 1.349\n",
    "        iqr = (stats.scoreatpercentile(x, 75) - stats.scoreatpercentile(x, 25)) / normalizer\n",
    "        std_dev = np.std(x, axis=0, ddof=1)\n",
    "        return np.minimum(std_dev, iqr) if iqr > 0 else std_dev\n",
    "    sigma = _select_sigma(data)\n",
    "    n = len(data)\n",
    "    return 0.9 * sigma * n ** (-0.2)\n",
    "\n",
    "def bw_mlcv(data: np.ndarray, k):\n",
    "    \"\"\"\n",
    "    Ref: https://rdrr.io/cran/kedd/src/R/MLCV.R\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    x = np.linspace(np.min(data), np.max(data), n)\n",
    "    def mlcv(h):\n",
    "        fj = np.zeros(n)\n",
    "        for j in range(n):\n",
    "            for i in range(n):\n",
    "                if i == j: continue\n",
    "                fj[j] += k((x[j] - data[i]) / h)\n",
    "            fj[j] /= (n - 1) * h\n",
    "        return -np.mean(np.log(fj[fj > 0]))\n",
    "    h = optimize.minimize(mlcv, 1)\n",
    "    if np.abs(h.x[0]) > 10:\n",
    "        return bw_scott(data)\n",
    "    return h.x[0]\n",
    "\n",
    "def kde(data, k=None, h=None, x=None):\n",
    "    \"\"\"Kernel Density Estimation.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Data.\n",
    "        k (function): Kernel function.\n",
    "        h (float): Bandwidth.\n",
    "        x (np.ndarray, optional): Grid. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Kernel density estimation.\n",
    "    \"\"\"\n",
    "    if x is None:\n",
    "        x = np.linspace(np.min(data), np.max(data), 1000)\n",
    "    if h is None:\n",
    "        h = bw_silverman(data)\n",
    "    if k is None:\n",
    "        k = kernel('gaussian')\n",
    "    n = len(data)\n",
    "    kde = np.zeros_like(x)\n",
    "    for j in range(len(x)):\n",
    "        for i in range(n):\n",
    "            kde[j] += k((x[j] - data[i]) / h)\n",
    "        kde[j] /= n * h\n",
    "    return kde\n",
    "\n",
    "data = [\n",
    "    ('Normal', make_data_normal),\n",
    "    ('Bimodal (Normal)', make_data_binormal),\n",
    "    ('Bimodal (Exp)', make_data_exp),\n",
    "    ('Bimodal (Uniform)', make_data_uniform)\n",
    "]\n",
    "kernels = [\n",
    "    ('Gaussian', kernel('gaussian')),\n",
    "    ('Epanechnikov', kernel('epanechnikov')),\n",
    "    ('Cosine', kernel('cosine')),\n",
    "    ('Linear', kernel('linear'))\n",
    "]\n",
    "bw_algorithms = [\n",
    "    ('Scott', bw_scott),\n",
    "    ('Silverman', bw_silverman),\n",
    "    ('MLCV', bw_mlcv),\n",
    "]\n",
    "mses = []\n",
    "\n",
    "def run_kde(ax, data, kernel):\n",
    "    x, dist = data[1]()\n",
    "    x_plot = np.linspace(np.min(x) * 1.05, np.max(x) * 1.05, 1000)\n",
    "    ax.grid(True)\n",
    "    ax.fill_between(x_plot, dist(x_plot), fc='silver', alpha=0.5)\n",
    "    ax.plot(x, np.full_like(x, -0.02), '|k', markeredgewidth=1)\n",
    "    ax.hist(x, density=True, alpha=0.2, bins=20, rwidth=0.9)\n",
    "    for bw in bw_algorithms:\n",
    "        if bw[0] == 'MLCV':\n",
    "            h = bw[1](x, kernel[1])\n",
    "        else:\n",
    "            h = bw[1](x)\n",
    "        x_kde = kde(x, kernel[1], h=h, x=x_plot)\n",
    "        mse = np.mean((dist(x_plot) - x_kde) ** 2)\n",
    "        mses.append({\n",
    "            'data': data[0],\n",
    "            'kernel': kernel[0],\n",
    "            'bw_algorithm': bw[0],\n",
    "            'h': round(h, 5),\n",
    "            'mse': round(mse * 1000, 5), # To make differences more noticable\n",
    "        })\n",
    "        ax.plot(x_plot, x_kde, linewidth=1, label='$h_{\\mathrm{' + bw[0] + '}} = ' + str(round(h, 5)) + '$')\n",
    "    ax.legend(loc='best', fontsize='small')\n",
    "    ax.set_title(f'{data[0]}, {kernel[0]}')\n",
    "\n",
    "fig, axs = plt.subplots(len(data), len(kernels), figsize=(16, 12))\n",
    "\n",
    "for i, d in enumerate(data):\n",
    "    for j, k in enumerate(kernels):\n",
    "        run_kde(axs[i, j], d, k)\n",
    "    for bw in bw_algorithms:\n",
    "        avg_h = np.mean([m['h'] for m in mses if m['data'] == d[0] and m['bw_algorithm'] == bw[0]])\n",
    "        avg_mse = np.mean([m['mse'] for m in mses if m['data'] == d[0] and m['bw_algorithm'] == bw[0]])\n",
    "        mses.append({\n",
    "            'data': d[0],\n",
    "            'kernel': '-',\n",
    "            'bw_algorithm': bw[0],\n",
    "            'h': round(avg_h, 5),\n",
    "            'mse': round(avg_mse, 5),\n",
    "        })\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig('eval.pdf')\n",
    "pd.DataFrame(mses).to_csv('eval.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f6466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,4):\n",
    "    for grd in range(1,9):\n",
    "        for unidad in [\"ICU\", \"SDU_WARD\"]:\n",
    "            # ---- 1. Filter your dataset\n",
    "            tl_u = tl[tl[\"UNIDAD\"].isin([\"ICU\", \"OR\", \"SDU_WARD\"])]\n",
    "            v1 = tl_u[(tl_u[\"UNIDAD\"] == unidad) & (tl_u[\"MS_GRD\"] == grd) & (tl_u[\"HOSPITAL\"] == f\"Hospital_{i}\")]\n",
    "            # v1 = tl_u[(tl_u[\"UNIDAD\"] == unidad) & (tl_u[\"MS_GRD\"] == grd)]\n",
    "\n",
    "            # ---- 2. Get LOS value counts\n",
    "            vector = v1[\"LOS\"].value_counts().reset_index().sort_values(by=\"LOS\")\n",
    "            vector.columns = [\"LOS\", \"count\"]\n",
    "\n",
    "            # ---- 3. Build raw data from LOS values and their frequencies\n",
    "            los = np.array(vector[\"LOS\"])\n",
    "            ocurrencias = np.array(vector[\"count\"])\n",
    "            raw_data = np.repeat(los, ocurrencias)\n",
    "\n",
    "\n",
    "            # ---- 4. Definir los bins de 12 horas\n",
    "            bin_width = 12\n",
    "            max_los = np.max(raw_data)\n",
    "            edges = np.arange(0, max_los + bin_width, bin_width)\n",
    "            midpoints = (edges[:-1] + edges[1:]) / 2  # midpoint of (a, b] is (a + b) / 2\n",
    "    \n",
    "\n",
    "            # ---- 5. Meter datos a bins individuales (0 - 12], (12 - 24], ...)\n",
    "            # agrega 0 si no hay datos para ese bin\n",
    "            bin_indices = np.digitize(raw_data, edges, right=True)\n",
    "            hist = np.array([(bin_indices == i).sum() for i in range(1, len(edges))])\n",
    "\n",
    "#--------------------------------Hasta aca esta bien----------------------------------------\n",
    "\n",
    "            # ---- 6. Fit log-normal distribution to the raw data\n",
    "            shape, loc, scale = stats.lognorm.fit(raw_data, floc=0)\n",
    "\n",
    "\n",
    "\n",
    "            # # ---- Optimización\n",
    "            # def chi2_objective(params, hist, edges, total_count):\n",
    "            #     shape, loc, scale = params\n",
    "            #     # Ensure parameters are valid\n",
    "            #     if shape <= 0 or scale <= 0:\n",
    "            #         return np.inf\n",
    "            #     cdf_low = stats.lognorm.cdf(edges[:-1], shape, loc, scale)\n",
    "            #     cdf_high = stats.lognorm.cdf(edges[1:], shape, loc, scale)\n",
    "            #     expected_probs = cdf_high - cdf_low\n",
    "            #     expected_counts = expected_probs * total_count\n",
    "\n",
    "            #     # Mask for bins with enough expected values\n",
    "            #     mask = expected_counts >= 5\n",
    "            #     if not np.any(mask):\n",
    "            #         return np.inf\n",
    "            #     obs = hist[mask]\n",
    "            #     exp = expected_counts[mask]\n",
    "                \n",
    "            #     # Normalize expected to match sum of obs\n",
    "            #     exp *= obs.sum() / exp.sum()\n",
    "\n",
    "            #     chi2 = np.sum((obs - exp)**2 / exp)\n",
    "            #     return chi2\n",
    "            # initial_shape, initial_loc, initial_scale = stats.lognorm.fit(raw_data, floc=0)\n",
    "            # # Minimize the chi-square\n",
    "            # result = minimize(\n",
    "            #     chi2_objective,\n",
    "            #     x0=[initial_shape, initial_loc, initial_scale],\n",
    "            #     args=(hist, edges, len(raw_data)),\n",
    "            #     bounds=[(1e-5, None), (0, None), (1e-5, None)],\n",
    "            #     method='L-BFGS-B'\n",
    "            # )\n",
    "            # # Extract optimized parameters\n",
    "            # opt_shape, opt_loc, opt_scale = result.x\n",
    "            # # print(f\"Optimized shape={opt_shape:.4f}, loc={opt_loc:.4f}, scale={opt_scale:.2f}\")\n",
    "            # shape = opt_shape\n",
    "            # loc = opt_loc\n",
    "            # scale = opt_scale\n",
    "\n",
    "\n",
    "            # # ---- 7. Compute expected counts per bin using log-normal CDF\n",
    "            # cdf_low = stats.lognorm.cdf(edges[:-1], shape, loc, scale)\n",
    "            # cdf_high = stats.lognorm.cdf(edges[1:], shape, loc, scale)\n",
    "            # expected_probs = cdf_high - cdf_low\n",
    "            # # Acumulo todo lo que queda de la cola en el ultimo bin\n",
    "            # # expected_probs[-1] = 1 - (sum(expected_probs) - expected_probs[-1])\n",
    "            # expected_counts = expected_probs * len(raw_data)\n",
    "\n",
    "\n",
    "            # # ---- 9. Chi-square test (only where expected counts ≥ 5)\n",
    "            # mask = expected_counts >= 5\n",
    "            # obs = hist[mask]\n",
    "            # exp = expected_counts[mask]\n",
    "\n",
    "            # # display(obs)\n",
    "            # # display(exp)\n",
    "\n",
    "            # # Normalize expected counts to match the total of observed\n",
    "            # if exp.sum() > 0:\n",
    "            #     exp *= obs.sum() / exp.sum()\n",
    "\n",
    "            # # Chi-square test\n",
    "            # if np.any(mask):\n",
    "            #     chi2, p = stats.chisquare(f_obs=obs, f_exp=exp)\n",
    "            #     if p > 0.05:\n",
    "            #         print(f\"Chi² = {chi2:.2f}, p-value = {p:.10f}, Unidad={unidad}, Hospital={i}, GRD={grd}\")\n",
    "                    \n",
    "            #         # ---- 8. Plot observed vs. expected\n",
    "            #         plt.figure(figsize=(12, 6))\n",
    "            #         plt.bar(midpoints, hist, width=bin_width - 2, alpha=0.6, label='Observed (12h bins)')\n",
    "            #         plt.plot(midpoints, expected_counts, 'r--o', label='Log-normal expected')\n",
    "            #         plt.xlabel('Length of Stay (hours)')\n",
    "            #         plt.ylabel('Frequency')\n",
    "            #         plt.title(f'LOS Histogram vs Log-normal Fit\\nUnidad={unidad}, Hospital={i}, GRD={grd}')\n",
    "            #         plt.grid(True)\n",
    "\n",
    "            #         # Curva continua\n",
    "            #         x_vals = np.linspace(1, max_los, 500)\n",
    "            #         pdf_vals = stats.lognorm.pdf(x_vals, s=shape, loc=loc, scale=scale)\n",
    "            #         pdf_scaled = pdf_vals * len(raw_data) * bin_width\n",
    "            #         plt.plot(x_vals, pdf_scaled, 'g-', label='Log-normal PDF (smoothed)')\n",
    "\n",
    "            #         plt.legend()\n",
    "            #         plt.show()\n",
    "            #         break\n",
    "\n",
    "                    \n",
    "            # else:\n",
    "            #     print(\"Chi-square test skipped: not enough expected counts ≥ 5.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7931e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla para los OR\n",
    "\n",
    "tabla = {\n",
    "    1: [[\"MS_GRD\", \"LOS = 12\", \"LOS = 24\"]],\n",
    "    2: [[\"MS_GRD\", \"LOS = 12\", \"LOS = 24\"]],\n",
    "    3: [[\"MS_GRD\", \"LOS = 12\", \"LOS = 24\"]]\n",
    "}\n",
    "\n",
    "for i in range(1,4):\n",
    "    for grd in range(1,9):\n",
    "        for unidad in [\"OR\"]:\n",
    "            # ---- 1. Filter your dataset\n",
    "            tl_u = tl[tl[\"UNIDAD\"].isin([\"ICU\", \"OR\", \"SDU_WARD\"])]\n",
    "            v1 = tl_u[(tl_u[\"UNIDAD\"] == unidad) & (tl_u[\"MS_GRD\"] == grd) & (tl_u[\"HOSPITAL\"] == f\"Hospital_{i}\")]\n",
    "            # v1 = tl_u[(tl_u[\"UNIDAD\"] == unidad) & (tl_u[\"MS_GRD\"] == grd)]\n",
    "\n",
    "            # ---- 2. Get LOS value counts\n",
    "            vector = v1[\"LOS\"].value_counts().reset_index().sort_values(by=\"LOS\")\n",
    "            vector[\"%\"] = vector[\"count\"] / vector[\"count\"].sum()\n",
    "            vector1 = vector[[\"LOS\", \"%\"]].reset_index()\n",
    "\n",
    "            los_12 = 0\n",
    "            los_24 = 0\n",
    "            for index, row in vector1.iterrows():\n",
    "                if row[\"LOS\"] == 12:\n",
    "                    los_12 = row[\"%\"]\n",
    "                elif row[\"LOS\"] == 24:\n",
    "                    los_24 = row[\"%\"]\n",
    "\n",
    "            tabla[i].append([grd, round(los_12,5), round(los_24,5)])\n",
    "                    \n",
    "            \n",
    "\n",
    "    display(tabla[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138ffe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasar a tablas de latex\n",
    "for key, h in tabla.items():\n",
    "\n",
    "    texto = f\"\"\"\n",
    "\\\\begin{{table}}[H]\n",
    "    \\\\centering\n",
    "    \\\\begin{{tabular}}{{ccc}}\n",
    "        \\\\toprule\n",
    "        MS\\_GRD & LOS = 12 & LOS = 24 \\\\\\\\\n",
    "        \\\\midrule\n",
    "        {h[1][0]} & {h[1][1]} & {h[1][2]} \\\\\\\\\n",
    "        {h[2][0]} & {h[2][1]} & {h[2][2]} \\\\\\\\\n",
    "        {h[3][0]} & {h[3][1]} & {h[3][2]} \\\\\\\\\n",
    "        {h[4][0]} & {h[4][1]} & {h[4][2]} \\\\\\\\\n",
    "        {h[5][0]} & {h[5][1]} & {h[5][2]} \\\\\\\\\n",
    "        {h[6][0]} & {h[6][1]} & {h[6][2]} \\\\\\\\\n",
    "        {h[7][0]} & {h[7][1]} & {h[7][2]} \\\\\\\\\n",
    "        {h[8][0]} & {h[8][1]} & {h[8][2]} \\\\\\\\\n",
    "        \\\\bottomrule\n",
    "    \\\\end{{tabular}}\n",
    "    \\\\caption{{Probabilidad LOS en OR para Hospital: {key}}}\n",
    "    \\\\label{{tab:Probabilidad LOS en OR para Hospital: {key}}}\n",
    "\\\\end{{table}}\n",
    "    \"\"\"\n",
    "\n",
    "    print(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6592e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segundo intento (quedo pesimo)\n",
    "\n",
    "# for i in range(1,4):\n",
    "# for grd in range(1,9):\n",
    "# for unidad in [\"ICU\", \"SDU_WARD\"]:\n",
    "\n",
    "unidad = \"ICU\"  # Cambiar unidad\n",
    "grd = 1\n",
    "\n",
    "# ---- 1. Filtra los datos para obtener LOS y cantidad de ocurrencias\n",
    "tl_u = tl[tl[\"UNIDAD\"].isin([\"ICU\", \"OR\", \"SDU_WARD\"])]\n",
    "# v1 = tl_u[(tl_u[\"UNIDAD\"] == unidad) & (tl_u[\"MS_GRD\"] == grd) & (tl_u[\"HOSPITAL\"] == f\"Hospital_{i}\")]\n",
    "v1 = tl_u[(tl_u[\"UNIDAD\"] == unidad) & (tl_u[\"MS_GRD\"] == grd)]\n",
    "\n",
    "# ---- 2. Calcular la cantidad de veces que se repite ese LOS\n",
    "vector = v1[\"LOS\"].value_counts().reset_index().sort_values(by=\"LOS\")\n",
    "vector.columns = [\"LOS\", \"count\"]\n",
    "\n",
    "# ---- 3. Pasarlo a formato para histograma\n",
    "los = np.array(vector[\"LOS\"])\n",
    "ocurrencias = np.array(vector[\"count\"])\n",
    "raw_data = np.repeat(los, ocurrencias) # multiplica LOS por cantidad de repeticiones\n",
    "\n",
    "# ---- 4. Definir los bins de 12 horas (para ajustar la curva)\n",
    "bin_width = 12\n",
    "max_los = np.max(raw_data) # Elige el maximo de LOS\n",
    "edges = np.arange(0, max_los + bin_width, bin_width) # Define los bins\n",
    "midpoints = (edges[:-1] + edges[1:]) / 2  # Define puntos medios de los bins\n",
    "\n",
    "# ---- 5. Meter datos a bins individuales (0 - 12], (12 - 24], ...) agrega 0 si no hay datos para ese bin\n",
    "bin_indices = np.digitize(raw_data, edges, right=True)\n",
    "hist = np.array([(bin_indices == i).sum() for i in range(1, len(edges))])\n",
    "bin_edges = edges\n",
    "bin_upper_bounds = bin_edges[1:]\n",
    "\n",
    "# ---- 6. Ajustar distribución a los datos en bins de 12 horas\n",
    "shape, loc, scale = stats.lognorm.fit(raw_data, floc = 0)\n",
    "\n",
    "# # Grafico para la curva suave log_normal\n",
    "# x = np.linspace(min(bin_upper_bounds), max(bin_upper_bounds), 1000)\n",
    "# y = stats.lognorm.pdf(x, shape, loc, scale) * len(raw_data) * bin_width\n",
    "# plt.hist(raw_data, bins=bin_edges, alpha=0.6, color='g', label=\"Histogram\")\n",
    "# plt.plot(x, y, label=\"Fitted Log-normal\", color='r')\n",
    "# plt.xlabel('Value')\n",
    "# plt.ylabel('Density')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# Define interval of interest\n",
    "  # for example\n",
    "N = len(raw_data)\n",
    "maximo = int(raw_data[-1])\n",
    "maximo = int(maximo/12)\n",
    "expected = []\n",
    "for i in range(0, maximo):\n",
    "    a = i * 12\n",
    "    b = (i + 1) * 12\n",
    "    p_interval = stats.lognorm.cdf(b, shape, loc, scale) - stats.lognorm.cdf(a, shape, loc, scale)\n",
    "    expected_occurrences = N * p_interval\n",
    "    expected.append(expected_occurrences)\n",
    "esperado = np.array(expected)\n",
    "observado = hist\n",
    "\n",
    "# Test de chi cuadrado\n",
    "mask = esperado >= 5\n",
    "\n",
    "obs = observado[mask]\n",
    "exp = esperado[mask]\n",
    "\n",
    "exp *= obs.sum() / exp.sum()\n",
    "\n",
    "chi2, p = stats.chisquare(f_obs=obs, f_exp=exp)\n",
    "print(f\"chi2 = {chi2}, p = {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83902231",
   "metadata": {},
   "outputs": [],
   "source": [
    "los = [12, 24, 36, 48, 60, 72]\n",
    "count = [5, 12, 8, 6, 4, 2]\n",
    "\n",
    "def continuous(los, count):\n",
    "    nueva_lista = []\n",
    "    lista_cuartos = []\n",
    "    for i in range(len(los)):\n",
    "        # for j in range(count[i]):\n",
    "        #     nueva_lista.append(los[i])\n",
    "        nueva_lista.append(round((count[i])/4, 2))\n",
    "    for cuarto in nueva_lista:\n",
    "        lista_cuartos.append(cuarto)\n",
    "        lista_cuartos.append(cuarto * 2)\n",
    "        lista_cuartos.append(cuarto)\n",
    "    \n",
    "    uniones = []\n",
    "    for i in range(0, len(lista_cuartos), 3):\n",
    "        if i == 0:\n",
    "            uniones.append(lista_cuartos[i])\n",
    "            uniones.append(lista_cuartos[i + 1])\n",
    "        elif i == len(lista_cuartos) - 3:\n",
    "            uniones.append(lista_cuartos[i] + lista_cuartos[i - 1])\n",
    "            uniones.append(lista_cuartos[i + 1])\n",
    "            uniones.append(lista_cuartos[i + 2])\n",
    "        else:\n",
    "            uniones.append(lista_cuartos[i] + lista_cuartos[i - 1])\n",
    "            uniones.append(lista_cuartos[i + 1])\n",
    "    \n",
    "    count = uniones.copy()\n",
    "\n",
    "    new_los = []\n",
    "    for i in range(len(uniones)):\n",
    "        if i == 0:\n",
    "            new_los.append(6)\n",
    "        elif i == len(uniones) - 1:\n",
    "            ultimo = new_los[-1]\n",
    "            new_los.append(ultimo + 6)\n",
    "        else:\n",
    "            nuevo = new_los[-1] + 6\n",
    "            new_los.append(nuevo)\n",
    "    \n",
    "    return new_los, count\n",
    "\n",
    "uniones, count = continuous(los, count)\n",
    "\n",
    "print(uniones)\n",
    "print(count)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80040f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros para cuando sea parte de una función\n",
    "plot = True\n",
    "bandwidth = 0.82\n",
    "###################### Datos a analizar ######################\n",
    "tl_u = tl[tl[\"UNIDAD\"].isin([\"ICU\", \"OR\", \"SDU_WARD\"])]\n",
    "v1 = tl_u[(tl_u[\"UNIDAD\"] == \"SDU_WARD\") & (tl_u[\"MS_GRD\"] == 1) & (tl_u[\"HOSPITAL\"] == f\"Hospital_{1}\")]\n",
    "vector = v1[\"LOS\"].value_counts().reset_index().sort_values(by=\"LOS\")\n",
    "vector.columns = [\"LOS\", \"count\"]\n",
    "los = np.array(vector[\"LOS\"])/12 \n",
    "ocurrencias = np.array(vector[\"count\"])\n",
    "raw_data = np.repeat(los, ocurrencias)\n",
    "##############################################################\n",
    "\n",
    "# Estimador del kernerl density discreto #####################\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth).fit(raw_data[:, None])\n",
    "##############################################################\n",
    "\n",
    "if plot:\n",
    "    ###################### Plot ##################################\n",
    "    # Histograma de los datos, tantos bins como el valor maximo de los datos\n",
    "    plt.figure(figsize=(10, 3)) # tamaño grafico\n",
    "    plt.hist(raw_data, bins=int(raw_data[-1] - (raw_data[0] - 1)), density=True, alpha=0.6, color='g', edgecolor='black', label='Histogram')\n",
    "\n",
    "    # pdf del KDE ajustado a los datos\n",
    "    x_vals = np.linspace(min(raw_data), max(raw_data), 1000)\n",
    "    log_dens = kde.score_samples(x_vals[:, None])\n",
    "    kde_pdf = np.exp(log_dens)\n",
    "    plt.plot(x_vals, kde_pdf, color='black', label=f'KDE (Bandwidth = {round(bandwidth,2)})')\n",
    "    # puntos de las intersecciones\n",
    "    x_vals_puntos = np.linspace(int(min(raw_data)), int(max(raw_data)), int(max(raw_data)) - int(min(raw_data)) + 1)\n",
    "    log_dens_puntos = kde.score_samples(x_vals_puntos[:, None])\n",
    "    kde_pdf_puntos = np.exp(log_dens_puntos)\n",
    "    plt.scatter(x_vals_puntos, kde_pdf_puntos, color='red', label=f'KDE intersecciones')\n",
    "\n",
    "\n",
    "    # Leyendas y nombres\n",
    "    plt.xlabel('LOS')\n",
    "    plt.ylabel('Densidad')\n",
    "    plt.title('Histograma y KDE discreto de LOS')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    ##############################################################\n",
    "\n",
    "metricas, _ = calculate_kde_metrics(los, ocurrencias, kde)\n",
    "display(pd.DataFrame(metricas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a07b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puntos de intersección entre data y kde (en los enteros: 1, 2, 3 ...)\n",
    "x_vals = np.linspace(int(min(los)), int(max(los)), int(max(los)))\n",
    "log_dens = kde.score_samples(x_vals[:, None])\n",
    "kde_pdf = np.exp(log_dens)\n",
    "\n",
    "# Almaceno el los y sus ocurrencias en un diccionario\n",
    "dict_temporal = {}\n",
    "for i in range(len(los)):\n",
    "    dict_temporal[int(los[i])] = ocurrencias[i]\n",
    "\n",
    "# Genero ocurrencias nuevamente pero agregando un cero en los intervalos sin ocurrencias\n",
    "ocurrencias_con_cero = []\n",
    "for i in range(1, int(max(los)) + 1):\n",
    "    ocurrencias_con_cero.append(dict_temporal.get(i, 0))\n",
    "ocurrencias_con_cero = np.array(ocurrencias_con_cero)\n",
    "\n",
    "# Calculo las métricas para el kde (que es el unico que voy a usar)\n",
    "def get_metrics(y_true, y_pred):\n",
    "    prediccion = y_pred * (y_true.sum() / y_pred.sum()) # reescala predicciones para el test\n",
    "    chi2 = chisquare(f_obs=y_true, f_exp=prediccion)\n",
    "    ks_stat = ks_2samp(y_true, y_pred)\n",
    "    cc= pearsonr(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        'Chi2': {\"Value\": round(chi2.statistic, 3), \"p-value\": round(chi2.pvalue, 3)},\n",
    "        'KS': {\"Value\": round(ks_stat.statistic, 3), \"p-value\": round(ks_stat.pvalue, 3)},\n",
    "        'CC': {\"Value\": round(cc.statistic, 3), \"p-value\": round(cc.pvalue, 3)},\n",
    "        'R2': {\"Value\": round(r2, 3), \"p-value\": \"NA\"},\n",
    "        'RMSE': {\"Value\": round(rmse, 3), \"p-value\": \"NA\"},\n",
    "        'MAE': {\"Value\": round(mae, 3), \"p-value\": \"NA\"}\n",
    "    }\n",
    "\n",
    "# Llamo a la función, se utilizan ocurrencias, no probabilidades\n",
    "metrics = get_metrics(ocurrencias_con_cero, kde_pdf*ocurrencias.sum())\n",
    "\n",
    "# Se muestran las metricas\n",
    "display(pd.DataFrame(metrics))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
